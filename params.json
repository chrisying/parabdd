{
  "name": "ParaBDD",
  "tagline": "Parallel Binary Decision Diagrams for Efficient Model Checking",
  "body": "## Summary\r\n\r\nWe created a parallel binary decision diagram (BDD) library using CilkPlus which allows us to find and count satisfying assignments to arbitrary boolean expressions efficiently. Our implementation achieves ___ speedup on ___ cores and beats an existing BDD library by over an order of magnitude.\r\n\r\n## Background\r\n\r\nBDD's are an efficient data structure for representing arbitrary boolean expressions. Using canonicity rules and duplicate detection, we can represent an exponentially growing data structure in manageable memory. The result is a directed acyclic graph that looks like the following graph generated by our library for 5-queens.\r\n\r\n![](http://www.chrisying.net/parabdd/images/nqueens.dot.svg)\r\n\r\nThe user interactions with the library by declaring elementary BDD's and composing them together with standard boolean operations. The library is as simple as:\r\n\r\n    Bdd a(1);\r\n    Bdd b(2);\r\n    Bdd c = a & b;\r\n\r\n    // Gives one satisfying assignment to the expression (returns <1:true, 2:true>)\r\n    unordered_map<Var, bool> map = c.one_sat();\r\n    \r\n    // Count the number of satisfying assignments (returns 1 for a & b)\r\n    int num = c.count_sat();\r\n\r\nThe following mechanisms are required to generate the BDD:\r\n* An efficient and multi-threaded hash table for storing BDD's so that each unique \"subgraph\" is only ever generated once.\r\n* Another hash table for caching the results of intermediate operations for constructing the BDD and doing other operations.\r\n* A set of logically consistent \"rewrite\" rules that allows us to enforce a canonical form on the structure of BDD's.\r\n\r\nMost existing BDD libraries, including the well-know libraries [BuDDy](http://buddy.sourceforge.net/manual/main.html) and [CUDD](http://vlsi.colorado.edu/~fabio/CUDD/cudd.pdf), have efficient single-threaded implementations for the first two points and have a decent set of canonicity rules. However, according to the [literature](http://essay.utwente.nl/61650/1/thesis_Tom_van_Dijk.pdf) that we reviewed, there are huge opportunities for improvement on all three fronts:\r\n* We can use a lock-free or fine-grained locking hash table to allow multi-threaded access to the uniques table and the cache.\r\n* We can use fork-join parallelism to speed up the construction and count SAT operations since they are mostly recursive \"divide-and-conquer\"-like algorithms.\r\n* We can decrease the number of unique elements by utilizing complement edges and it's associated rewrite rules.\r\n\r\n## Approach\r\n\r\nWe will first discuss the hash table we implemented. We initially wanted to implement a lock-free hash table to store unique BDD's as well as the cache. However, it turns out that the keys to our hash table would have be be at least 16 bytes long, which means we would have to use double-length compare-and-swap (CMPXCHG16B), which is unsupported by the Xeon Phi, which immediately killed that option.\r\n\r\nInstead, we created a fine-grained open-addressing hash table. Our hash table takes an arbitrary length key and hashes it using the [MurmurHash3](https://github.com/aappleby/smhasher) hashing scheme. After hashing the key to a bucket, we linearly probe for an open bin and acquire a lock on that bin before inserting the element. We initialize the hash table to a size that will ensure a low number of collisions.\r\n\r\nNext, we will discuss the fork-join parallelism. One of the key operations for constructing a BDD is an \"if-then-else\" (ITE) expansion, which is used for composing any two BDD's together. ITE requires recursively traversing through the BDD graph, calling ITE independently twice per run. Since the two recursive calls are independent, they can be called in parallel. However, since we don't have enough cores to run all ITE calls in parallel together, we have to implement some sort of work queue in order to efficiently utilize all the cores as well as respect the execution order (the two recursive ITE calls have to finish before returning).\r\n\r\nThis model of computation is known as fork-join parallelism, which forks to spawn jobs that should be added to a work queue and joins to combine the results from forked jobs. Unfortunately, C++ semantics make it hard to implement arbitrary continuations (required for general fork-join) without compiler support. We choose to use [CilkPlus](https://www.cilkplus.org/), which provides parallel keywords for fork-join semantics.\r\n\r\nLastly, we will discuss the additional opportunities for canonicity rules. The BuDDy library represents BDD's without complemented edges. However, according to the literature, we can effectively represent a BDD and its complement with the same BDD by implementing a complex set of rewrite and canonicity rules. To do this with minimal memory footprint, we used a bit hack in order to encode the complement state in the existing pointer itself.\r\n\r\n## Results\r\n\r\n## Future Work\r\n\r\nThere are many additional optimizations that exist in current libraries that were not possible to implement in the scope of this project. Primarilty, the key optimizations are:\r\n* Reordering rules to optimize the order that variables are constructed to minimize the BDD complexity\r\n* Garbage collection for unused BDD's to reduce the peak memory footprint\r\n\r\nIn addition, in the process of designing the library, we encountered a few areas for improvement:\r\n* Fork-join rescheduling in order to eliminate the rare chance of duplicated work (e.g. if another worker is currently doing a job, that job will never be done by another worker)\r\n* Granularity control in order to reduce the overhead of the fork-join parallelism (e.g. with a system like PASL)\r\n\r\n## References\r\n\r\nVan Dijk, Tom, Alfons Laarman, and Jaco Van De Pol. \"Multi-core BDD operations for symbolic reachability.\" Electronic Notes in Theoretical Computer Science 296 (2013): 127-143.\r\n\r\nSomenzi, Fabio. \"CUDD: CU Decision Diagram Package Release 3.0. 0.\" (2015).\r\n\r\n## List of Work\r\n\r\n\"equal work was performed by both project members.\"\r\n\r\n***\r\n\r\n# Proposal\r\n## Summary\r\nBinary Decision Diagrams (BDDs) are commonly used in industry for model checking, such as in computer-aided design. We will implement a parallel BDD library that is optimized for many-core machines, such as the Xeon Phi.\r\n\r\n## Background\r\nA BDD is a data structure used to represent boolean functions in a graph structure. BDD's can be used to model state transformations and evaluate the validity of systems. For example, one use case for BDD's is verifying the correctness of concurrent programs to ensure that multiple threads cannot access a critical section at the same time. In general, evaluation for model checking involves taking an initial BDD and a set of transition rules and computing the \"reachibility\" of the states in an iterative fashion.\r\nThough there are existing existing packages that perform BDD evaluation, none of them utilize modern multi-core processors like the Xeon Phi. In order to take advantage of multi-core processors, we need to be able to represent a BDD in a structure that multiple processors can access and modify at one. In addition, we need to memoize the computation of different parts of the BDD to prevent duplicate work. These two goals can be met with a lockfree global hash, which reduces synchronization and communication costs compared to a lock-based data structure.\r\n\r\n## Challenge\r\nOne of the big conceptual challenges for this problem is that neither of us have worked with BDD's before. To fully understand the project, we will first need to explore the literature covering BDD's and the algorithms that deal with evaluating them. Since there are no publicly available parallel BDD evaluators yet, we will have to base our code on research papers which have not actually implemented the any realistic system in practice.\r\nAnother big challenge is getting a fast and correct lockfree hash table. Though there are numerous papers and existing lockfree hash tables, we will likely have to implement our own version to support memory management, resizing, and probing in the way that we need. Specifically, we will need to write our own garbage collection to delete unused BDD nodes to keep the memory footprint at a reasonable size.\r\n\r\n## Resources\r\nWe will utilize the Xeon Phi’s on the latedays cluster.\r\nWe will be starting from scratch, though there is a similar sequential library that is commonly used, [BuDDy](http://buddy.sourceforge.net/manual/main.html).\r\nOur main literary resource will be [Tom van Dijk’s master’s thesis](https://www.google.com/urlhttp://essay.utwente.nl/61650/1/thesis_Tom_van_Dijk.pdf), which presents a survey over the field of BDDs and provides proofs of several BDD operations.\r\nWe would greatly benefit from access to a Knight’s Landing Xeon Phi, but Professor Kayvon has told us this is impossible.\r\n\r\n## Goals and Deliverables\r\nOur project will have two main deliverables which are largely independent:\r\n* First, we will have a standalone lockfree hash table implementation that supports memory management and resizing. This hash table will be benchmarked against existing locking and lockfree hash tables to ensure that our implementation is competitive with the state-of-the-art systems that exist.\r\n* Second, we will have an system that takes an initial state (modeled as a BDD) and a transition relation (as a BDD) as input and evaluates the reachibility of the state. Users will be able to interface with our system through an API that allows them to manually construct the BDD's that they wish to evaluate. In order to demo this part of the project, we will write a few example programs that will use BDD's to check the correctness of some system (for example: deadlock checking, or n-Queens problem).\r\n\r\n## Platform Choice\r\nTo properly demonstrate the parallelism and scalability of our library, which we intend to be able to handle BDDs with millions of nodes, we need to use hardware with many cores.  The Xeon Phi provides a machine with 60 cores, along with a simple target to optimize for.\r\nThis library will be implemented in C++14, as this is a reasonable language that people seem to like, and will be targeted at all standards-compliant operating systems, with preference given to Linux if we run into issues.\r\n\r\n## Schedule\r\nWeek 1:\r\nImplement a sequential implementation of BDDs and ensure correctness.\r\n\r\nWeek 2:\r\nImplement a parallel hash table for memoization and reduction purposes.\r\n\r\nWeek 3:\r\nModify the sequential implementation to be parallel.  Debug until correct.\r\n\r\nWeek 4:\r\nPerformance tweaking.\r\n\r\nWeek 5:\r\nCreation of cool looking test cases/demo.\r\n\r\n***\r\n\r\n# Checkpoint\r\n## Work So Far\r\nSo far, we have created a detailed specification for our project including the user-facing API and the internal structures we will be using. We have also mostly completed a sequential implementation of the BDD library, which allows the user to construct BDD's using logical operators and do simple analysis like \"one SAT\" (find one satisfying assignment for the BDD) and \"count SAT\" (find number of satisfying assignments for the BDD). Our BDD's are maintained in a canonical form in order to save memory.\r\n\r\nWe also gave considerable thought into designing the parallel hash table in order to implement the unique BDD table and the cache. This task turned out to be much more difficult than we initially expected for a few reasons:\r\n* The hardware we had access to did not have 128-bit compare-and-swap, which makes it significantly harder to implement a lockfree hash table since we need to do compare-and-swaps on a pointer plus a counter.\r\n* We realized that we would have to implement our own garbage collector over the cache, which means we would need to maintain reference counters in order to keep the memory footprint manageable.\r\n\r\nWe also spent a good amount of time trying to implement a fork-join parallelism model to handle work distribution. However, this turned out to be a dead-end because implementing such a system is significantly out-of-scope for the difficulty of a 418 project, especially when combined with the rest of the project. We ended up deciding to use the Cilk library instead.\r\n\r\n## Updated Schedule\r\n4/22:\r\n* Verify the correctness of the sequential algorithm by implementing n-Queens (Chris)\r\n* Finish work on the sequential RelProd algorithm, which is used for model checking (Chris)\r\n* Finish the lockfree hash table for the cache and uniques table (Matt)\r\n4/25:\r\n* Finish the reachability algorithm for model checking (Chris)\r\n* Finish cache and uniques table by adapting the lockfree hash table to the needs of the library (Matt)\r\n4/29:\r\n* Write some test cases to verify the correctness of the model checker (Chris)\r\n* Finish adding fork-join parallelism via Cilk (Matt)\r\n5/2:\r\n* Benchmark against \r\nBuDDy and test the library to ensure correctness, debug as necessary (both)\r\n* Gather data about the parallelism of our system, including the speedup and memory footprint (both)\r\n5/6:\r\n* Final fine tuning (both)\r\n* Finish creating a final presentation for our project (both)\r\n* Begin practicing for the presentation (both)\r\n5/9:\r\n* Finish final writeup (both)\r\n\r\n## Potential issues\r\nThough we have given it much thought and done background research to ensure that it is reasonable, we have yet to actually test the code on a Xeon Phi using Cilk. Our hope is that Cilk will be enough for our fork-join needs though we a bit concerned that we will hit unforeseen roadblocks while actually implementing it in the Phi's. Our backup plan is to run the code on the GHC machines, which will give some benchmarks, though we may not be able to observe as large of a speedup.\r\n\r\nAnother big concern is the fact that we are somewhat blindly implementing BDD algorithms according to a few research papers that we have read. So far, things have been going reasonably well, but if we eventually get a correctness bug, it will be incredibly difficult to debug since the algorithms presented in the papers are quite complex and presented in a simplistic fashion. We will try to debug this by manually inspecting the BDD's, though this becomes unfeasible when we try to scale up. This may require help from Professor Bryant if we cannot solve the issue.\r\n\r\n## Updated deliverables\r\nFor our final presentation we hope to show:\r\n* Implementing a simple BDD with our library to show how easy it is to use.\r\n* Performance benchmarks for creating and operating on large BDD's compared against existing sequential libraries.\r\n* STRETCH: Demonstration of a model checker for verifying the correctness of a piece of parallel code (i.e. deadlock detection).\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}